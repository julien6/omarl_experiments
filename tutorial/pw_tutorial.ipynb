{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# Use PRAHOM Pettingzoo Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "## Introducing PRAHOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKOCZlhUgXVK"
   },
   "source": [
    "### PRAHOM context\n",
    "\n",
    "Given a Markovian model of an environment where agents have to collaborate to reach a common goal, solving it is just finding a right joint-policy (we can view as a set of rules for all agents) to achieve it.\n",
    "\n",
    "*Multi-Agent Reinforcement Learning* allows converging to a joint-policy that reach the common goal. Yet a regular MARL pipeline does not cover two main concerns:\n",
    " - The \"trained\" joint-policy is often difficult to understand raising explainability and safety issues\n",
    " - It is difficult to satisfy some extra constraints we want sometimes in order to meet some design requirements or to help agents to converge faster within a restricted search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About PRAHOM\n",
    "The Partial *Relation between Agents' History and Organizational Model* (PRAHOM) is an algorithmic approach aiming to link MARL with the $\\mathcal{M}OISE^+$ Organizational Model. It seeks to explicating and handling the collective emergent behavior of a regular MARL pipeline by leveraging agents' histories as a common ground between MARL and Organizational Specifications (OS).\n",
    "\n",
    "Indeed, PRAHOM enables:\n",
    " - Training of a joint-policy under OS viewed as constraints.\n",
    " - Inferring organizational specifications from a \"trained\" joint-policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typical use case\n",
    "For example, if you are given a set of agents acting for a football team that has to score a goal against an enemy team, you have several choices to implement the joint-policy depending on the level of constraining:\n",
    " - In the least constrained case, you may choose to let them learn the best way to organize themselves without any indication $\\rightarrow$ It is regular MARL where agents may find some very empiric over-fitted solution in the beginning, and it generally takes a lot of time for a joint-policy to become more general by training on differently generated environments. In that case, some collective structured patterns may appear that are naturally fitted on the long term: roles such as goalkeeper, defender, attacker... may appear in someway.\n",
    " - In the most constrained case, you may choose to tell what agents must do in every circumstances $\\rightarrow$ Indeed it is just creating a hand-crafted joint-policy and there is no learning.\n",
    " - Between these two ends, you may also choose agents to learn within a restricted search space you think to be relevant to speed up the converging towards a partially defined joint-policy $\\rightarrow$ Some agents may be forced to be close to the goal cages while other may be forced to go to the front or forcing: you can expect some roles to appear to attack and some other ones to defend.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRAHOM Pettingzoo wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PRAHOM Pettingzoo Wrapper* is an additional layer to augment a Pettingzoo environment with functionalities to help a user apply PRAHOM.\n",
    "Its use can be summarized according to 4 main steps as follow:\n",
    "\n",
    "1) [optional: Determining some known OS and associate them some joint-histories]\n",
    "2) [optional: Constrain agent to some of the known OS]\n",
    "3) Proceed to the MARL pipeline under optionally defined constraints and avoiding over-fitting\n",
    "4) Inferring the refined OS after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, we walk through these 4 steps in the simple *Moving Company* environment to highlight rather individual specifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNrNXKI7bINP"
   },
   "source": [
    "If you haven't installed the following dependencies, run:\n",
    "1) Launch \"create_venv.sh\": it will create a \"tuto_env\" virtual python environment and install it as a jupyter kernel\n",
    "2) Use the \"tuto_env\" kernel for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may be required for virtual display\n",
    "# !sudo apt-get install xvfb\n",
    "# !sudo apt install python3-pip\n",
    "# !sudo apt-get install python-is-python3\n",
    "# !sudo apt-get install python3.10-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./tuto_env/lib/python3.10/site-packages (3.9.0)\n",
      "Requirement already satisfied: SuperSuit in ./tuto_env/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in ./tuto_env/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pettingzoo[butterfly,classic] in ./tuto_env/lib/python3.10/site-packages (1.24.3)\n",
      "Requirement already satisfied: PyQt5 in ./tuto_env/lib/python3.10/site-packages (5.15.10)\n",
      "Requirement already satisfied: stable-baselines3 in ./tuto_env/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in ./tuto_env/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: tensorboard in ./tuto_env/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: pillow>=8 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: tinyscaler>=1.2.6 in ./tuto_env/lib/python3.10/site-packages (from SuperSuit) (1.2.7)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in ./tuto_env/lib/python3.10/site-packages (from SuperSuit) (0.29.1)\n",
      "Requirement already satisfied: pygame==2.3.0 in ./tuto_env/lib/python3.10/site-packages (from pettingzoo[butterfly,classic]) (2.3.0)\n",
      "Requirement already satisfied: pymunk==6.2.0 in ./tuto_env/lib/python3.10/site-packages (from pettingzoo[butterfly,classic]) (6.2.0)\n",
      "Requirement already satisfied: chess==1.9.4 in ./tuto_env/lib/python3.10/site-packages (from pettingzoo[butterfly,classic]) (1.9.4)\n",
      "Requirement already satisfied: rlcard==1.0.5 in ./tuto_env/lib/python3.10/site-packages (from pettingzoo[butterfly,classic]) (1.0.5)\n",
      "Requirement already satisfied: shimmy[openspiel]>=1.2.0 in ./tuto_env/lib/python3.10/site-packages (from pettingzoo[butterfly,classic]) (1.3.0)\n",
      "Requirement already satisfied: cffi>1.14.0 in ./tuto_env/lib/python3.10/site-packages (from pymunk==6.2.0->pettingzoo[butterfly,classic]) (1.16.0)\n",
      "Requirement already satisfied: termcolor in ./tuto_env/lib/python3.10/site-packages (from rlcard==1.0.5->pettingzoo[butterfly,classic]) (2.4.0)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in ./tuto_env/lib/python3.10/site-packages (from PyQt5) (12.13.0)\n",
      "Requirement already satisfied: PyQt5-Qt5>=5.15.2 in ./tuto_env/lib/python3.10/site-packages (from PyQt5) (5.15.2)\n",
      "Requirement already satisfied: pandas in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: torch>=1.13 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./tuto_env/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./tuto_env/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./tuto_env/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: six>1.9 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./tuto_env/lib/python3.10/site-packages (from gymnasium>=0.28.1->SuperSuit) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./tuto_env/lib/python3.10/site-packages (from gymnasium>=0.28.1->SuperSuit) (4.12.2)\n",
      "Requirement already satisfied: open-spiel>=1.2 in ./tuto_env/lib/python3.10/site-packages (from shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (1.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (8.9.2.26)\n",
      "Requirement already satisfied: sympy in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.12.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2.20.5)\n",
      "Requirement already satisfied: triton==2.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2.3.1)\n",
      "Requirement already satisfied: fsspec in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./tuto_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./tuto_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tuto_env/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tuto_env/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: pycparser in ./tuto_env/lib/python3.10/site-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly,classic]) (2.22)\n",
      "Requirement already satisfied: pip>=20.0.2 in ./tuto_env/lib/python3.10/site-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (22.0.2)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in ./tuto_env/lib/python3.10/site-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (0.1.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in ./tuto_env/lib/python3.10/site-packages (from open-spiel>=1.2->shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (23.2.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./tuto_env/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: PyYAML in ./tuto_env/lib/python3.10/site-packages (from ml-collections>=0.1.1->open-spiel>=1.2->shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (6.0.1)\n",
      "Requirement already satisfied: contextlib2 in ./tuto_env/lib/python3.10/site-packages (from ml-collections>=0.1.1->open-spiel>=1.2->shimmy[openspiel]>=1.2.0->pettingzoo[butterfly,classic]) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stable-baselines3[extra] in ./tuto_env/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.20 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: pandas in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.29.1)\n",
      "Requirement already satisfied: torch>=1.13 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.9.0)\n",
      "Requirement already satisfied: cloudpickle in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: psutil in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (5.9.8)\n",
      "Requirement already satisfied: pygame in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.3.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.17.0)\n",
      "Requirement already satisfied: shimmy[atari]~=1.3.0 in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.10.0.82)\n",
      "Requirement already satisfied: pillow in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (10.3.0)\n",
      "Requirement already satisfied: rich in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (13.7.1)\n",
      "Requirement already satisfied: tqdm in ./tuto_env/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.66.4)\n",
      "Requirement already satisfied: requests in ./tuto_env/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.32.3)\n",
      "Requirement already satisfied: click in ./tuto_env/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in ./tuto_env/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./tuto_env/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./tuto_env/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in ./tuto_env/lib/python3.10/site-packages (from shimmy[atari]~=1.3.0->stable-baselines3[extra]) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: six>1.9 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (59.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./tuto_env/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
      "Requirement already satisfied: triton==2.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: filelock in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (2.20.5)\n",
      "Requirement already satisfied: sympy in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12.1)\n",
      "Requirement already satisfied: fsspec in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.0)\n",
      "Requirement already satisfied: networkx in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./tuto_env/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./tuto_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]) (12.5.40)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (4.53.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tuto_env/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tuto_env/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./tuto_env/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./tuto_env/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
      "Requirement already satisfied: importlib-resources in ./tuto_env/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./tuto_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./tuto_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./tuto_env/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./tuto_env/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./tuto_env/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./tuto_env/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./tuto_env/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard in ./tuto_env/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: six>1.9 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./tuto_env/lib/python3.10/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./tuto_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in ./tuto_env/lib/python3.10/site-packages (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./tuto_env/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in ./tuto_env/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./tuto_env/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: triton==2.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: fsspec in ./tuto_env/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./tuto_env/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./tuto_env/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: filelock in ./tuto_env/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: jinja2 in ./tuto_env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./tuto_env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./tuto_env/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./tuto_env/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./tuto_env/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./tuto_env/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: networkx in ./tuto_env/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./tuto_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./tuto_env/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: numpy in ./tuto_env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./tuto_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./tuto_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /home/julien/Documents/ThÃ¨se/omarl_experiments/custom_envs/dist/custom_envs-0.1.tar.gz\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gymnasium in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (0.29.1)\n",
      "Requirement already satisfied: matplotlib in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (3.9.0)\n",
      "Requirement already satisfied: numpy in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (1.26.4)\n",
      "Requirement already satisfied: pettingzoo in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (1.24.3)\n",
      "Requirement already satisfied: pillow in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (10.3.0)\n",
      "Requirement already satisfied: pygame in ./tuto_env/lib/python3.10/site-packages (from custom-envs==0.1) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./tuto_env/lib/python3.10/site-packages (from gymnasium->custom-envs==0.1) (4.12.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./tuto_env/lib/python3.10/site-packages (from gymnasium->custom-envs==0.1) (3.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./tuto_env/lib/python3.10/site-packages (from gymnasium->custom-envs==0.1) (0.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (4.53.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (3.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tuto_env/lib/python3.10/site-packages (from matplotlib->custom-envs==0.1) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in ./tuto_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->custom-envs==0.1) (1.16.0)\n",
      "Using legacy 'setup.py install' for custom-envs, since package 'wheel' is not installed.\n",
      "Installing collected packages: custom-envs\n",
      "  Attempting uninstall: custom-envs\n",
      "    Found existing installation: custom-envs 0.1\n",
      "    Uninstalling custom-envs-0.1:\n",
      "      Successfully uninstalled custom-envs-0.1\n",
      "  Running setup.py install for custom-envs ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed custom-envs-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!source tuto_env/bin/activate\n",
    "%pip install matplotlib SuperSuit numpy pettingzoo[classic,butterfly] PyQt5 stable-baselines3 scikit-learn tensorboard --timeout 86400;\n",
    "%pip install stable-baselines3[extra] --timeout 86400;\n",
    "%pip install tensorboard --timeout 86400;\n",
    "%pip install pyvirtualdisplay --timeout 86400;\n",
    "%pip install torch torchvision --timeout 86400;\n",
    "%pip install ./../custom_envs/dist/*.tar.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T13:55:17.447507Z",
     "iopub.status.busy": "2023-12-22T13:55:17.447082Z",
     "iopub.status.idle": "2023-12-22T13:55:20.751418Z",
     "shell.execute_reply": "2023-12-22T13:55:20.750703Z"
    },
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "import pyvirtualdisplay\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, List, Set, Tuple, Union\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "from pettingzoo.utils.env import ActionType, AECEnv, AgentID, ObsType\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym / PettingZoo environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Showcase environment : *Moving Company*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About *Moving Company* within PettingZoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In RL/MARL, an environment represents the task or problem to be solved.\n",
    "\n",
    "Moving Company (MCY) is a two-dimensional grid game where mover employees have to bring a package from a cell to a final cell. They are free to move up, left, down, and right in the white cells. They can pick up or drop down the package in the drop zone (yellow cells). The white cells are empty and the grey cells represent walls. The game ends when the package is dropped in the final cell. The environment is fully discrete, vectorized. Agents' observations are the 3x3 grid cells surrounding an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we instantiate a \"turn-based\" (aka Agent Environment Cycle - AEC) environment with 9 x 9 grid cells and `'rgb_aray'` mode.\n",
    "\n",
    "We also instantiate a parallel environment with the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-22T13:55:20.865902Z",
     "iopub.status.busy": "2023-12-22T13:55:20.865429Z",
     "iopub.status.idle": "2023-12-22T13:55:20.896073Z",
     "shell.execute_reply": "2023-12-22T13:55:20.895421Z"
    },
    "id": "pYEz-S9gEv2-"
   },
   "outputs": [],
   "source": [
    "from custom_envs.movingcompany import moving_company_v0\n",
    "\n",
    "aec_env = moving_company_v0.env(size=9, render_mode=\"rgb_array\")\n",
    "\n",
    "parallel_env = moving_company_v0.parallel_env(size=9, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIHYVBkuvPNw"
   },
   "source": [
    "You can render this AEC environment to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aec_env.reset()\n",
    "Image.fromarray(aec_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result with parallel env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env.reset()\n",
    "Image.fromarray(parallel_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the instantiated agents in the environment (same in AEC and parallel environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_env.agents)\n",
    "aec_env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, actions space is a 0 to 7 discrete space, one-hot encoding is the following:\n",
    " - 0: do nothing\n",
    " - 1: go up\n",
    " - 2: go down\n",
    " - 3: go left\n",
    " - 4: go right\n",
    " - 5: take\n",
    " - 6: drop\n",
    "\n",
    "We can print the action space for `'agent_0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aec_env.action_space(\"agent_0\"))\n",
    "parallel_env.action_space(\"agent_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, observations space is a 3 x 3 grid (flattened) surrounding an agent, one-hot encoding for each cell is the following:\n",
    " - 0: wall\n",
    " - 1: air\n",
    " - 2: agent\n",
    " - 3: agent carrying a box\n",
    " - 4: drop zone\n",
    " - 5: box\n",
    "\n",
    "We can print the observation space for `'agent_0'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_env.observation_space(\"agent_0\"))\n",
    "print(aec_env.observation_space(\"agent_0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the AEC environment, agents apply their actions one after one in a cyclic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aec_env.reset()\n",
    "\n",
    "# First step\n",
    "aec_env.step(5) # 'agent_0' takes the box\n",
    "aec_env.step(4) # 'agent_1' goes right\n",
    "aec_env.step(2) # 'agent_2' goes down\n",
    "\n",
    "# Second step\n",
    "aec_env.step(2) # 'agent_0' goes down\n",
    "aec_env.step(0) # 'agent_1' does nothing\n",
    "aec_env.step(0) # 'agent_2' does nothing\n",
    "\n",
    "Image.fromarray(aec_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AEC environment, for a given agent the `last()` methods return a 5-tuple for its turn. It is composed of:\n",
    " - the current observation\n",
    " - the reward according to its last action\n",
    " - the truncation (max iteration reached for instance)\n",
    " - the done (has the game finished?)\n",
    " - the info (here contains action masks: here `'agent_0'` cannot go left, go right, take nor drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation, reward, termination, truncation, info = env.last()\n",
    "print(aec_env.agent_selection, \"receives data for its turn:\")\n",
    "pprint(aec_env.last())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel environment, agents apply their action simultaneously at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_env.reset()\n",
    "\n",
    "# First step\n",
    "parallel_env.step({\"agent_0\": 5, \"agent_1\": 4, \"agent_2\": 2})\n",
    "\n",
    "# Second step\n",
    "parallel_env.step({\"agent_0\": 2, \"agent_1\": 0, \"agent_2\": 0})\n",
    "\n",
    "Image.fromarray(aec_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel environment, the `step()` method returns a 5-tuple composed of the data for all agent for the next step. It is composed of:\n",
    " - next observation for all agent\n",
    " - the reward for all agent\n",
    " - truncations for all agent\n",
    " - the dones for all agent\n",
    " - the infos for all agent (that may contain action masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(parallel_env.step({\"agent_0\": 2, \"agent_1\": 3, \"agent_2\": 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: 1) Defining some OS and linking them to histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we may already have some known information as for what the resulting joint-policy should look like.\n",
    "For instance, we may want some agents not to play some dangerous actions at some point, or we may already\n",
    "known a promising organizational form to reach the goal even though it is not yet precisely defined.\n",
    "\n",
    "This what we are going to do leveraging links between a formal description through OS of the organization and the\n",
    "expected behavior through joint-histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some OS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first key concept is the Organizational Specification (OS) which represents a particular expected individual,\n",
    "social or collective aspect. Organizational specifications comprise roles, links, compatibilities, goals, plans, missions, etc.\n",
    "\n",
    "An OS is a component of the organizational model among these:\n",
    "- {SS: roles, links, compatibilities, {sub-groups: SS}, role_cardinality, sub-group_cardinality}\n",
    "- FS: {SCH:{goals, plans, missions, mission_to_goals, mission_to_agent_cardinality}, social_preference_order}\n",
    "- DS: permissions, obligations\n",
    "\n",
    "A single OS can be described within the organizational model with optional empty components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove after finalizing PRAHOM package #############################\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "# sys.path.append(module_path + \"/prahom_wrapper\")\n",
    "sys.path.append(module_path)\n",
    "############################################################################\n",
    "\n",
    "from prahom_wrapper.organizational_model import organizational_model, structural_specifications\n",
    "\n",
    "role_0_os = organizational_model(\n",
    "    structural_specifications=structural_specifications(roles=[\"role_0\"], role_inheritance_relations=None, root_groups=None), functional_specifications=None, deontic_specifications=None)\n",
    "\n",
    "pprint(role_0_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `role_0_os` defines any organization where \"role_0\" is available for agents.\n",
    "Consequently, `role_0_os` only deals with a role so it is just a singe OS.\n",
    "\n",
    "Note:\n",
    " - \"None\" value indicate any value can be accepted. It can be used if you do not know additional information regarding the ones you already provided\n",
    " - If you want to express no value, you must choose {} or [] depending on the expected value format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_01_os = organizational_model(\n",
    "    structural_specifications=structural_specifications(roles=[\"role_0\", \"role_1\"], role_inheritance_relations={\"role_1\": [\"role_0\"]}, root_groups=None), functional_specifications=None, deontic_specifications=None)\n",
    "pprint(role_01_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `role_01_os` defines any organization where \"role_0\" and \"role_1\" are available for agents and we provide a known information\n",
    "about these roles which is \"role_1\" inherits from \"role_0\".\n",
    "If you had not do it, you could have let it to \"None\" and it would be then possible to infer it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some joint-histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second key concept is the joint-history...\n",
    "Let's recap:\n",
    "- for an agent, a policy associates an observation to an action for an agent to reach a goal.\n",
    "- for an agent, a history is a tuple describing sequentially all the observation-action at each step until the end of an episode\n",
    "- for several agents, a joint-policy associates all the agents' observation to the agents' actions at each step enabling them to\n",
    "    collectively reach their goal. It can be seen as a tuple of policies or a relation taking a vector of observation and associating\n",
    "    a vector of actions.\n",
    "- for several agents, a joint-history is a tuple of the agents' histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation and an action are simple objects whose type is user's responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import observation, action\n",
    "\n",
    "obs1: observation = np.array([1, 0, 0, 4, 2, 1, 0, 0, 0])\n",
    "act1: action = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to some complex observation or action types, we also define associated string shortcuts `action_label` and `observation_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import observation_label, action_label\n",
    "\n",
    "obs_label1: observation_label = \"o01\"\n",
    "act_label1: action_label = \"a1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users are required to define a mapping from any used label to its corresponding object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_obj: Dict[Union[observation_label, action_label], object] = {\n",
    "    \"a0\": 0,\n",
    "    \"a1\": 1,\n",
    "    \"a2\": 2,\n",
    "    \"a3\": 3,\n",
    "    \"a4\": 4,\n",
    "    \"a5\": 5,\n",
    "    \"a6\": 6,\n",
    "\n",
    "    \"o01\": np.array([0, 1, 0, 0, 2, 0, 0, 1, 0]),  # 0 -> 1\n",
    "    \"o02\": np.array([0, 5, 0, 0, 2, 0, 0, 1, 0]),  # 1 -> 5\n",
    "    \"o03\": np.array([0, 4, 0, 0, 3, 0, 0, 1, 0]),  # 2 -> 2\n",
    "    \"o04\": np.array([0, 1, 0, 0, 3, 0, 0, 1, 0]),  # 2 -> 2\n",
    "    \"o05\": np.array([0, 1, 0, 0, 3, 0, 0, 4, 1]),  # 3 -> 6\n",
    "    \"o06\": np.array([0, 1, 0, 0, 3, 0, 0, 4, 2]),  # 3 -> 6\n",
    "    \"o07\": np.array([0, 1, 0, 0, 2, 0, 0, 5, 1]),  # -> 0\n",
    "    \"o08\": np.array([0, 1, 0, 0, 2, 0, 0, 5, 2]),  # -> 0\n",
    "    \"o09\": np.array([0, 1, 0, 0, 2, 0, 0, 4, 3]),  # -> 0\n",
    "    \"o010\": np.array([0, 1, 0, 0, 2, 0, 0, 4, 1]),  # -> 0\n",
    "\n",
    "    \"o11\": np.array([1, 0, 0, 5, 2, 1, 0, 0, 0]),  # 1 -> 5\n",
    "    \"o12\": np.array([2, 0, 0, 5, 2, 1, 0, 0, 0]),  # 1 -> 5\n",
    "    \"o13\": np.array([1, 0, 0, 4, 3, 1, 0, 0, 0]),  # 2 -> 4\n",
    "    \"o14\": np.array([2, 0, 0, 4, 3, 1, 0, 0, 0]),  # 2 -> 4\n",
    "    \"o15\": np.array([0, 0, 0, 1, 3, 1, 0, 0, 0]),  # 2 -> 4\n",
    "    \"o16\": np.array([0, 0, 0, 1, 2, 1, 0, 0, 0]),  # 0 -> 3\n",
    "    \"o17\": np.array([0, 0, 1, 1, 3, 4, 0, 0, 0]),  # 3 -> 6\n",
    "    \"o18\": np.array([0, 0, 2, 1, 3, 4, 0, 0, 0]),  # 3 -> 6\n",
    "    \"o19\": np.array([0, 0, 1, 1, 2, 5, 0, 0, 0]),  # -> 0\n",
    "    \"o110\": np.array([0, 0, 2, 1, 2, 5, 0, 0, 0]),  # -> 0\n",
    "    \"o111\": np.array([1, 0, 0, 4, 2, 1, 0, 0, 0]),  # -> 0\n",
    "    \"o112\": np.array([3, 0, 0, 4, 2, 1, 0, 0, 0]),  # -> 0\n",
    "    \"o113\": np.array([0, 0, 1, 1, 2, 4, 0, 0, 0]),  # -> 0\n",
    "    \"o114\": np.array([0, 0, 3, 1, 2, 4, 0, 0, 0]),  # -> 0\n",
    "\n",
    "    \"o21\": np.array([0, 1, 0, 0, 2, 0, 1, 5, 0]),  # 2 -> 5\n",
    "    \"o22\": np.array([0, 1, 0, 0, 2, 0, 2, 5, 0]),  # 2 -> 5\n",
    "    \"o23\": np.array([0, 1, 0, 0, 3, 0, 1, 4, 0]),  # 3 -> 1\n",
    "    \"o25\": np.array([0, 4, 0, 0, 2, 0, 0, 1, 0]),  # 1 -> 2\n",
    "    \"o26\": np.array([0, 1, 0, 0, 2, 0, 1, 4, 0]),  # -> 0\n",
    "    \"o27\": np.array([0, 1, 0, 0, 2, 0, 3, 4, 0])  # -> 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **history** is just a list of alternating `observation_label` and `action_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import history\n",
    "\n",
    "h1: history = [\"o02\", \"a1\", \"o03\", \"a2\", \"o04\", \"a3\", \"o05\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can define a set of histories using the `histories` class. It gathers several histories under the form of a specific oriented graph in a compact way. It is a convenient structure to be used for training and OS inferring.\n",
    "\n",
    "Users can create an empty `histories` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import histories\n",
    "\n",
    "hg = histories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a `history` described exhaustively or several ones as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = histories()\n",
    "hg.add_history([\"o01\", \"a0\", \"o01\", \"a1\", \"o01\", \"a1\", \"o01\", \"a1\", \"o02\"])\n",
    "hg.add_histories([[\"o01\", \"a0\", \"o01\"], [\"o02\", \"a1\", \"o03\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a graph plot representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, `ord_num_to_card` maps the crossing order number to a counter that represents the cardinality (always one for exhaustive description)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg1 = histories()\n",
    "hg1.add_history([\"o01\", \"a0\", \"o01\", \"a1\", \"o01\", \"a1\", \"o01\", \"a1\", \"o02\", \"a2\"])\n",
    "graph_plot_rgb = hg1.generate_graph_plot(render_rgba=True, transition_data=[\"ord_num_to_card\"])\n",
    "\n",
    "# transition_data values is:\n",
    "#  - ord_num_to_card (used for exhaustive description)\n",
    "\n",
    "# Image.fromarray(graph_plot_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a `pattern_histories` object.\n",
    "The pattern used is the following:\n",
    " - sequence = **[** *sequences or labels separated by \"* **,** *\" or \"* **|** \" **](** *multiplicity_lower_bound* **, ** multiplicity_upper_bound **)**\n",
    "   - sequential operator \",\": label_1,label_2 $\\rightarrow$ label_1 is played and then label_2\n",
    "   - choice operator \"|\": label_1|label_2 $\\rightarrow$ at least either label_1 or label_2 is played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, `ord_num_to_card` maps the crossing order number to a counter that represents the cardinality*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import histories, pattern_histories\n",
    "\n",
    "hg = histories()\n",
    "\n",
    "hg.add_pattern('[o0,a0,o1](5,7)')\n",
    "\n",
    "graph_plot = hg.generate_graph_plot(\n",
    "    render_rgba=True, transition_data=[\"ord_num_to_card\"])\n",
    "\n",
    "# Image.fromarray(graph_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until know we only defined subsets of histories by linking known observation to action and reversely.\n",
    "\n",
    "Sometimes, we may want to a subset of histories with unknown sub-sequences.\n",
    "For instance, we may want to force 'agent_0' to take the package when it is next to itself in the superior cell and let the rest of the next sequences free to be explored.\n",
    "\n",
    "We can do this with `pattern_histories` using special keywords:\n",
    "\n",
    "| Keyword     | Description|\n",
    "|------------|-------------------------------------------------------------|\n",
    "| `'#any_act'`      | any action in that sense an agent may choose a different action among available ones|\n",
    "| `'#any_obs'`      | any observation in that sense an agent may get a different observation among available ones|\n",
    "| `'#any_seq'`      | any sequence in that sense it may change each time an agent cross it. It is equivalent to `'[#any_obs,#any_act](1,*)'`|\n",
    "| `'#act_{`*id* `}'`      | a fixed unknown action *id* in that sense an agent would choose the same action each time it crosses to it|\n",
    "| `'#obs_{` *id* `}'`      | a fixed unknown observation *id* in that sense an agent would get the same observation each time it crosses to it|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each time an agent receives 'o02' (package in superior cell) it necessarily applies action 'a05' (take package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = histories()\n",
    "\n",
    "# o02 -> superior cell is package\n",
    "# a5  -> take package\n",
    "\n",
    "hg.add_pattern('[#any_seq[o02,a5](0,*)#any_seq](1,*)')\n",
    "\n",
    "graph_plot = hg.generate_graph_plot(\n",
    "    render_rgba=True, transition_data=[\"ord_num_to_card\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **joint-history** is a tuple where each component is a single history associated with an agent. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import joint_history\n",
    "\n",
    "jth: joint_history = {\n",
    "    'agent_0': ['o02', 'a5', 'o03', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o06', 'a6', 'o08', 'a0', 'o09', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010','a0'],\n",
    "    'agent_1': ['o111', 'a0', 'o111', 'a0', 'o111', 'a0', 'o111', 'a0', 'o111', 'a0', 'o111', 'a0', 'o112', 'a0', 'o12', 'a5', 'o14', 'a4', 'o15', 'a4', 'o15', 'a4', 'o15', 'a4', 'o15', 'a4', 'o18', 'a6', 'o110', 'a0', 'o114', 'a0', 'o113', 'a3', 'o16', 'a4', 'o113', 'a0', 'o113', 'a0', 'o113', 'a0', 'o113','a0'],\n",
    "    'agent_2': ['o25', 'a0', 'o25', 'a2', 'o01', 'a2', 'o01', 'a2', 'o01', 'a2', 'o01', 'a2', 'o26', 'a0', 'o26', 'a0', 'o26', 'a0', 'o26', 'a1', 'o01', 'a2', 'o26', 'a0', 'o26', 'a0', 'o27', 'a0', 'o22', 'a5', 'a1', 'o04', 'a1', 'o04', 'a1', 'o04', 'a1', 'o04', 'a1', 'o03', 'a6', 'o02','a0']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering a subset of $n$ **joint-history**, then each agent is associated to $n$ **histories**.\n",
    "So, we can consider a subset of joint-history as a **joint-histories** where each component is a subset of history or **histories**.\n",
    "We can define it with a `joint_histories` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import joint_histories\n",
    "\n",
    "jhs = joint_histories([\"agent_0\",\"agent_1\",\"agent_2\"])\n",
    "jhs.add_joint_history(jth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linking OS to joint-histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying PRAHOM's idea, is to hypothesize if there is a specific OS among the agents' implicit organization, then their\n",
    "joint-histories (obtained from a same joint-policy) should  match a specific pattern accordingly. Reversely, if some joint-histories\n",
    "obtained from a same joint-policy share some common features, then the agents' implicit organization should be described into OS\n",
    "defined according to these common features.\n",
    "\n",
    "Therefore, as we may already known some OS and their expected associated behaviors, we must define all of the possible joint-histories\n",
    "to describe the impact of an OS.\n",
    "It is important to note, the number of agent is undefined and we do not known what OS are associated to what agent.\n",
    "So, when defining associated joint-histories associated to an OS, we must be careful to keep it general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we known how to define a set of histories for a single agent, we would like to define a relation between each defined OS with a relation telling how the OS impact any subset of agents regarding their histories. It can formalized as such:\n",
    "\n",
    "$$osj: \\mathcal{OS} \\rightarrow \\mathcal{P}(\\mathcal{A}) \\times \\mathcal{P}(H_{joint})$$\n",
    "\n",
    "*(with $osj$ is OS to joint-history relation, $\\mathcal{OS}$ is the organizational specification set, $\\mathcal{A}$ the agent set, and $H_{joint}$ the joint-history set.)*\n",
    "\n",
    "It can be understood as for any $os \\in \\mathcal{OS}$, among $|\\mathcal{A}|$ agents only $n_{os}$ agents can be constrained to respect $os$.\n",
    "Consequently, their joint-histories would have to be in $H_{joint, os} \\in \\mathcal{P}(H_{joint})$ to respect $os$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another convenient way to see $osj$ is as a relation associating any $os \\in \\mathcal{OS}$ to another specific relation associating the OS with the fixed arity of $n_{os}$ agents that are constrained to:\n",
    "\n",
    "$$osaj: \\mathcal{OS} \\times \\mathcal{A}^{n_{os}} \\rightarrow H_{joint, \\mathcal{OS}}$$\n",
    "\n",
    "*(with $osaj$ associates an OS and some implied agents to the corresponding joint-histories)*\n",
    "\n",
    "And $osj: \\mathcal{OS} \\rightarrow \\mathcal{A} \\times osaj[\\mathcal{A}^{n_{OS}} \\times \\mathcal{OS}]$\n",
    "\n",
    "For instance, if an OS is just a \"role\", then it should be associated with relation $rh: \\mathcal{R} \\times \\mathcal{A} \\rightarrow H_{joint,\\mathcal{R}}$. Here $rh(\"agent_0\", \"role_0\")$ would give a joint-history where \"role_0\" is applied to \"agent_0\". Here, for the obtained joint-history only the history associated to \"role_0\" would be constrained to be in a history subset.\n",
    "\n",
    "Theoretically, we may define specific relations that belong to $osaj$ for each OS such as:\n",
    "  - roles: $raj(\\rho, ag) = H_{joint, \\mathcal{\\rho}}$\n",
    "  - links: $laj(l, ag_{1}, ag_{2}) = H_{joint,\\mathcal{l}}$\n",
    "  - compatibilities: $caj(c, ag_{1}, ag_{2}) = H_{joint, \\mathcal{c}}$\n",
    "  - ...\n",
    "\n",
    "The interest for these relations is to determine automatically the maximum of OS knowledge according to a given joint-history obtained after training by comparison with known relations. We consider OS except roles depend on roles to be properly defined. Indeed, we expect users to provide roles and the maximum of OS related to them when they link OS to joint-histories.\n",
    "\n",
    "Moreover, we only want to identify OS from joint-histories, we can restrict these relations only to roles because the rest of the OS depend on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the relation that associates a role to a joint-history using the `osj_relation` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.history_model import joint_histories\n",
    "from prahom_wrapper.relation_model import osj_relation\n",
    "\n",
    "hist = ['o02', 'a5', 'o03', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o06', 'a6', 'o08', 'a0', 'o09', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010']\n",
    "hh = histories()\n",
    "hh.add_history(hist)\n",
    "\n",
    "role_0 = organizational_model(\n",
    "    structural_specifications=structural_specifications(roles=[\"role_0\"], role_inheritance_relations=None, root_groups=None), functional_specifications=None, deontic_specifications=None)\n",
    "\n",
    "osjh = osj_relation([\"agent_0\", \"agent_1\", \"agent_2\"])\n",
    "\n",
    "# ----------------\n",
    "\n",
    "jhs0 = joint_histories([\"agent_0\",\"agent_1\",\"agent_2\"])\n",
    "jhs0.add_joint_histories({\n",
    "    \"agent_0\": hh,\n",
    "    \"agent_1\": None,\n",
    "    \"agent_2\": None\n",
    "})\n",
    "\n",
    "osjh.link_os(role_0, jhs0, agents=[\"agent_0\"])\n",
    "\n",
    "# ----------------\n",
    "\n",
    "jhs1 = joint_histories([\"agent_0\",\"agent_1\",\"agent_2\"])\n",
    "jhs1.add_joint_histories({\n",
    "    \"agent_0\": None,\n",
    "    \"agent_1\": hh,\n",
    "    \"agent_2\": None\n",
    "})\n",
    "\n",
    "osjh.link_os(role_0, jhs1, agents=[\"agent_1\"])\n",
    "\n",
    "# ----------------\n",
    "\n",
    "jhs2 = joint_histories([\"agent_0\",\"agent_1\",\"agent_2\"])\n",
    "jhs2.add_joint_histories({\n",
    "    \"agent_0\": None,\n",
    "    \"agent_1\": None,\n",
    "    \"agent_2\": hh\n",
    "})\n",
    "\n",
    "osjh.link_os(role_0, jhs2, agents=[\"agent_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous code was left for explanatory reasons. It is indeed equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = ['o02', 'a5', 'o03', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o04', 'a2', 'o06', 'a6', 'o08', 'a0', 'o09', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010', 'a0', 'o010', 'a0', 'o010', 'a1', 'o01', 'a2', 'o010']\n",
    "hh = histories()\n",
    "hh.add_history(hist)\n",
    "\n",
    "role_0 = organizational_model(\n",
    "    structural_specifications=structural_specifications(roles=[\"role_0\"], role_inheritance_relations=None, root_groups=None), functional_specifications=None, deontic_specifications=None)\n",
    "\n",
    "osjh2 = osj_relation([\"agent_0\", \"agent_1\", \"agent_2\"])\n",
    "osjh2.link_role(role_0, hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define 'role_0' that we associate with all possible combination of joint-histories for any agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: 2) Constraining agents to some OS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined a relation between an OS and all combinations of tuples containing the implied agents and the resulting joint-histories for all agents.\n",
    "\n",
    "To constrain an agent to some OS, we just have to select the tuple containing the concerned agent to get the joint-history subset that any agent's history should belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This can be done the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_agent_0_jths = osjh2.get_joint_histories(role_0, [\"agent_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can convert it into a `joint_policy_constraint` object to be passed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prahom_wrapper.policy_model import joint_policy_constraint\n",
    "\n",
    "jpc2 = joint_policy_constraint([role_agent_0_jths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Training under some constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "\n",
    "<td style=\"width: 50%;border-color:#FF000000\">\n",
    "We consider a given MARL algorithm that iteratively converges towards a joint-policy so that the joint-policy is updated at each step until a finite horizon. By default we choose <b>Proximal Policy Optimization</b> jointly with <b>StableBaseLine3</b> and <b>Optuna</b>.\n",
    "\n",
    "We consider that restricting the actions according to the organizational specifications at each step allows constraining the converged joint-policies to the ones that satisfy the organizational specifications. Relying on that principle we propose the algorithm presented in Algorithm 3.\n",
    "\n",
    "The proposed algorithm fits within a regular MARL context: joint-policy $\\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$. New training are launched until $it_{max}$ times to get more joint-policies to add in $s\\pi_{joint}$ as a final result.\n",
    "\n",
    "The proposed algorithm augments that framework by changing the way agents choose their actions to meet the expectations of the organizational specifications. Using the known relations $osr$ with agents' constraints $cons[\\mathcal{A}]$, $cons\\_act$ has to guess the most expected actions regarding the current $h_{joint,ep}$. In practice, a decision tree reconstructed from the $osr[cons[\\mathcal{A}]]$ allows getting the expected actions by following the specific episode joint-history $h_{joint,ep}$. These expected joint-actions $a_{joint,exp}$ are to be chosen by agents.\n",
    "\n",
    "Two modes are available to integrate these constraints:\n",
    "  - **correct-mode**: Correct any unexpected action by an expected one (such as a random sample). It aims to converge faster by reducing the search space with safety guarantees. Yet, it is external to the agent's policy;\n",
    "  - **penalize-mode**: Add a penalty to the reward if any wrong action has been made among agents. This mode aims to make agents \"learn\" to respect their constraints. Yet, no safety guarantee is ensured since agents only approximate their expected behavior.\n",
    "\n",
    "</td>\n",
    "<td style=\"width: 50%;border-color:#FF000000\">\n",
    "\n",
    "<center><img src=\"../../omarl_experiments/assets/images/algo_prahom_osr.png\" alt=\"drawing\" width=\"450\"/></center>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, this can done automatically the following way:\n",
    "\n",
    "(*Note: Training may take about one up to two hours depending on your configuration*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_envs.movingcompany import moving_company_v0\n",
    "from prahom_wrapper.prahom_wrapper import prahom_wrapper\n",
    "\n",
    "env = moving_company_v0.raw_env(render_mode=\"human\", size=10, seed=42)\n",
    "\n",
    "roles = organizational_model(\n",
    "    structural_specifications=structural_specifications(roles=[\"role_0\", \"role_1\", \"role2\"], role_inheritance_relations=None, root_groups=None), functional_specifications=None, deontic_specifications=None)\n",
    "jt_histories = joint_histories(env.possible_agents).add_joint_history(jth)\n",
    "\n",
    "osj_rel = osj_relation(env.possible_agents).link_os(\n",
    "    roles, jt_histories, env.possible_agents)\n",
    "\n",
    "roles_agents_pc = joint_policy_constraint(\n",
    "    [osj_rel.get_joint_histories(roles, env.possible_agents)])\n",
    "\n",
    "train_env = moving_company_v0.parallel_env(\n",
    "    render_mode=\"grid\", size=10, seed=42)\n",
    "eval_env = moving_company_v0.parallel_env(\n",
    "    render_mode=\"rgb_array\", size=10, seed=42)\n",
    "env = prahom_wrapper(env, osj_rel, roles_agents_pc, label_to_obj)\n",
    "\n",
    "env.train_under_constraints(\n",
    "    train_env=train_env, test_env=eval_env, total_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the trained model the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.test_trained_model()\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url='./moving_company_v0.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Inferring the refined OS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some OS using first the KOSIA approach and then the GOSIA one within a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_specs = env.generate_specs()\n",
    "pprint(raw_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KOSIA: Knowledge-based OS Identification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed **Knowledge-based Organizational Specification Identification Approach** (**KOSIA**) aims identifying some organizational specifications thanks to the currently known relations. KOSIA is aimed to be relevant when knowledge of the relations between histories and organizational specifications is significant enough to be used generally.\n",
    "For instance, we implemented KOSIA as a pattern-matching engine finding similar joint-histories to a given one hence finding the associated organizational specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOSIA: General OS Inferrence Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./../../omarl_experiments/assets/images/gosia_illustrative_view.png\" alt=\"drawing\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not possible to get all known associated organizational specifications, the proposed **General Organizational Specification Inference Approach** (GOSIA) suggests an empirical approach to infer the rest of them.\n",
    "GOSIA is based on some proposed definitions for each $\\mathcal{M}OISE^+$ organizational specification regarding joint-histories or other organizational specifications, to use suggested specific statistical, unsupervised learning techniques to infer them incrementally. The Figure 1 summarizes the five steps of GOSIA (represented as arrow labels) that are detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Infer roles and their inheritance\n",
    "\n",
    "We propose a role $\\rho$ is defined as a policy whose the associated histories of agents having adopted it all contain a common discontinuous sequence. We proposed a role $\\rho_2$ inherits $\\rho_2$ if the common discontinuous sequence of the histories associated with $\\rho_2$ is also contained in the $\\rho_1$'s one.\n",
    "From these definitions, GOSIA leverages hierarchical sequence clustering to find the longest common discontinuous sequences among agent's histories. Results can represented as a dendrogram. It enables inferring roles and inheritance relations, their respective relation with histories, and the current agents as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='./role_clustering.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Infer possible organizations\n",
    "\n",
    "We propose an organization is linked to only one set of all instantiable roles sharing closely similar inheritance relations. Indeed, considering two trained joint-policies $H_{joint,i,s,1}$ and $H_{joint,i,s,2}$, even though both achieve a goal relying on the roles $\\mathcal{R}_{ss,1}$ and $\\mathcal{R}_{ss,2}$ may be far from other. For instance, their roles may not use the same responsibility distribution.\n",
    "GOSIA uses a K-means algorithm to get the $q$ clusters of the vectorized $\\mathcal{IR}_{i}$ considered as organizations. The roles in the same cluster share the inheritance relations of the K-means' centroid $\\mathcal{IR}_j$. Indeed, they are representative general roles regarding all the similar roles adopted by agents of the same organization over all joint-histories.\n",
    "For the next steps, only one chosen organization and its related joint-histories are considered. When it exists it chooses an organization close to KOSIA's one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Infer links and sub-groups\n",
    "\n",
    "We propose two agents have a **social impact link** $(ag_1,ag_2, \\kappa, \\delta, f)$ with $h_1$ associated with $ag_1$ and $h_2$ associated with $ag_2$ if a sequence $h_{1,s}$ in $h_1$ is correlated at a $\\kappa \\in [0,1]$ index to another sequence $h_{2,s}$ in $h_2$ positioned at a relative delay $\\delta \\in [0,1]$ after the beginning of $h_{1,s}$, and these two correlated sequences are $f$ frequently present among all joint-policies.\n",
    "We consider two agents to be in the same group if there is a social impact link such as $f \\geq 0.9$. Considering that $\\kappa$ indicates the likeliness of an agent' sequence to impact another one and that $\\delta$ indicates the receiver's reactivity, we consider:\n",
    "  - an acquaintance link $(ag_1,ag_2,acq)$ is defined if there is a social impact link with $\\kappa \\geq 0.1$, $\\delta \\geq 0$, $f \\geq 0$;\n",
    "  - a communication link $(ag_1,ag_2,com)$ is defined if there is a social impact link with $\\kappa \\geq 0.3$, $\\delta \\geq 0$, $f \\geq 0$;\n",
    "  - an authority link $(ag_1,ag_2,aut)$ is defined if there is a social impact link with $\\kappa \\geq 0.9$, $\\delta \\geq 0.5$, $f \\geq 0$.\n",
    "\n",
    "GOSIA uses empirical techniques to compute a graph of the social impact links between agents. Frequency enables determining clusters as agents' groups and their associated roles. It enables inferring the acquaintance, communication, and authority links between roles. From the information concerning roles associated with groups, it is possible to infer whether links are intra-group or inter-group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Infer goals, plans, and missions\n",
    "\n",
    "We propose a sub-goal/goal is a set of common states that are reached following the histories of the successful agents.\n",
    "For each joint-history GOSIA computes the state transition graph that is merged into a general one. Measuring the distance between two vectorized states within K-means enables finding the clusters of trajectories that some agents may follow. Then, we sampled some sets of states for each trajectory as goals. For instance, one may choose the narrowest set of states in which agents collectively seem to transition at some point to achieve their goal. Otherwise, a balanced sampling over lower variance trajectories could be made. Knowing what goal belongs to what trajectory, GOSIA infers plans for choice and sequence only.\n",
    "\n",
    "This enables getting goals and plans at a global state but these goals could be indeed split into specific goals for each sub-group and agent. To do this, GOSIA conducts the same process replacing the states with the observations of the agents in the same sub-group for sub-groups, and observations for agents.\n",
    "\n",
    "We propose a mission as the set of sub-goals one or several agents are achieving.\n",
    "Knowing what shared goals are achieved by agents, GOSIA determines some sets of representative goals as missions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='./transition_goals.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Infer compatibilities, obligations, permissions, and cardinalities\n",
    "\n",
    "We propose an obligation is when an agent playing role $\\rho$ is achieving a mission's goals and no other one at some time constraints while an obligation is when an agent playing role $\\rho$ may achieve some other ones at some time constraints.\n",
    "GOSIA determines what agents' are associated with what mission and if they are restricted to some hence these are obligations or just permission\n",
    "\n",
    "We propose a compatibility $(\\rho_1,\\rho_2)$ is defined if an agent playing a history associated with $\\rho_1$ in a joint-history also plays a history associated with $\\rho_2$ in another joint-history. If this change operates only in the same group, then it is intra-group. Else it is inter-group.\n",
    "\n",
    "Finally, by counting the number of agents playing a role in each joint-history, each role's cardinality is computed. Similarly, GOSIA computes each sub-group's cardinality counting the inferred ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Perspective of complementary help from LLMs\n",
    "\n",
    "We started experimenting **EleutherAI** **gpt-neo-2.7B** pre-trained transformer using **Transformers**' **Huggingface** library. Within **PRAHOM\\_hos** it is intended to be automatically prompt-engineered with generated specific contextual descriptions of the environment, its functioning, and the $\\mathcal{M}OISE^+$ organizational specifications. This prompt-engineering is to guide the answers towards three main purposes:\n",
    "  - Labeling, tagging the inferred organizational specification from related joint-histories in a human-like manner;\n",
    "  - Giving specific human-like textual description about each organizational specification;\n",
    "  - Giving general human-like textual description of the whole organization.\n",
    "\n",
    "Despite, being at an early stage of use in our contribution, the first results obtained with LLMs show to be a promising research focus to assist the human understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers transformers[torch] transformers[tf-cpu] transformers[flax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://soulej:Tigrou_du_66@proxy-http.esisar.grenoble-inp.fr:3128'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://soulej:Tigrou_du_66@proxy-http.esisar.grenoble-inp.fr:3128'\n",
    "\n",
    "# import requests\n",
    "# import pypac\n",
    "\n",
    "# def request(method, url, **kwargs):\n",
    "#     with pypac.PACSession() as session:\n",
    "#         return session.request(method=method, url=url, **kwargs)\n",
    "\n",
    "# requests.request = request\n",
    "\n",
    "import requests\n",
    "import functools\n",
    "\n",
    "requests.request = functools.partial(requests, verify=False)\n",
    "\n",
    "import sys\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "action_to_description = {\n",
    "    \"a0\": \"does nothing\",\n",
    "    \"a1\": \"goes up\",\n",
    "    \"a2\": \"goes down\",\n",
    "    \"a3\": \"goes left\",\n",
    "    \"a4\": \"goes right\",\n",
    "    \"a5\": \"takes package\",\n",
    "    \"a6\": \"drops package\"\n",
    "}\n",
    "\n",
    "action_hist_description = {agent: \", then \".join([action_to_description[label] for label in labels if label in action_to_description.keys()]) for agent, labels in jth.items()}\n",
    "\n",
    "print(action_hist_description)\n",
    "\n",
    "for agent in jth.keys():\n",
    "\n",
    "    question = f'A person who {action_hist_description[agent]} is a '\n",
    "    print(question)\n",
    "    # Example: \"A person who takes package, then goes down, then goes down, then goes down, then goes down, then goes down, then drops package, then does nothing, then does nothing, then does nothing, then does nothing, then does nothing, then goes up, then goes down, then does nothing, then goes up, then goes down, then does nothing, then does nothing, then goes up, then goes down, then does nothing is a \"\n",
    "\n",
    "\n",
    "    # Generate answer with prompt\n",
    "    response = generator(question, max_length=400)\n",
    "\n",
    "    # Print answer\n",
    "    print(\"=\"*30)\n",
    "    print(response[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tuto_env",
   "language": "python",
   "name": "tuto_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
